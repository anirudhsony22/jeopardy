# -*- coding: utf-8 -*-
"""propositional_chunking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vdFMYoazZLTh6VuJCjUSfDZegj0kYjSG

**Get Files from google drive**
"""



from google.colab import drive
drive.mount('/content/drive')

# file_path = '/content/drive/MyDrive/jeopardy_project/nltk_chunk/'
# !cp -r /content/drive/MyDrive/jeopardy_project/nltk_chunk/nltk_chunking.jsonl /content/nltk_chunk/

import os
import time
import shutil
import glob

# while True:
for file in glob.glob('/content/emb*'):
    base_name = os.path.basename(file)
    dest_path = os.path.join('/content/drive/MyDrive/jeopardy_project/granular_bge/', base_name)
    if not os.path.exists(dest_path):
      shutil.move(file, dest_path)

"""**Define Pickle Utils and Load**

**Propositional Chunking**
"""

import os
import json
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm

# === Pickle stuff
from collections import defaultdict

def nested_defaultdict():
    return defaultdict(int)

def save_pickle(obj, path):
    with open(path, 'wb') as f:
        pickle.dump(obj, f)

def open_pickle(path):
    with open(path, 'rb') as f:
      return pickle.load(f)

# === Manual Config
model_name = 'BAAI/bge-base-en-v1.5'
jsonl_file = '/content/nltk_chunk/nltk_chunking.jsonl'
base_dir = '/content/drive/MyDrive/jeopardy_project/nltk_chunk/bge_chunking/'

# === Load model with GPU support
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")
model = SentenceTransformer(model_name, device=device)


# === Load JSONL input
chunk_ids = []
texts = []
metadata = []

with open(jsonl_file, 'r') as fin:
    for line in fin:
        entry = json.loads(line)
        text = entry.get("proposition_text", "").strip()
        if text:
            texts.append(text)
            chunk_ids.append(entry["prop_id"])
            metadata.append(entry)


# === Embed in Chunks
CHUNK_SIZE = 10000
total_chunks = (len(texts) + CHUNK_SIZE - 1) // CHUNK_SIZE

START_INDEX = 0
for chunk_index in tqdm(range(START_INDEX,total_chunks), desc="Embedding Chunks"):
    start = chunk_index * CHUNK_SIZE
    end = min(start + CHUNK_SIZE, len(texts))

    emb_file = os.path.join(base_dir, f'embeddings_chunk_{chunk_index}.npy')
    id_file = os.path.join(base_dir, f'metadata_chunk_{chunk_index}.pkl')

    if os.path.exists(emb_file) and os.path.exists(id_file):
        print(f"Skipping chunk {chunk_index} (already exists)")
        continue

    chunk_texts = texts[start:end]
    chunk_metadata = metadata[start:end]

    embeddings = model.encode(
        chunk_texts,
        batch_size=256,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=True
    )

    np.save(emb_file, embeddings)
    with open(id_file, 'wb') as f:
        pickle.dump(chunk_metadata, f)

    print(f"âœ… Saved: {emb_file}, {id_file}")

